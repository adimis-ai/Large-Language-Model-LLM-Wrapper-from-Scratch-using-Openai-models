{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdDME7gb0nY3ATKdc3ld3V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adimis-ai/Large-Language-Model-LLM-Wrapper-from-Scratch-using-Openai-models/blob/main/Large%20Language%20Model%20(LLM)%20Wrapper%20from%20Scratch%20using%20Openai%20models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "VBLvTBLxP32D"
      },
      "outputs": [],
      "source": [
        "#!pip install openai\n",
        "#!pip install --upgrade tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import tiktoken\n",
        "import time\n",
        "from typing import List, Union\n",
        "from pydantic import BaseModel"
      ],
      "metadata": {
        "id": "C4SmZWvcsQ9l"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"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\""
      ],
      "metadata": {
        "id": "Hkzjm1gYsSI8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatMessage(BaseModel):\n",
        "    role: str\n",
        "    content: str"
      ],
      "metadata": {
        "id": "QB2vjYgLsUZ4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions\n",
        "def count_tokens(text, model_name=\"gpt-3.5-turbo\"):\n",
        "    encoding = tiktoken.encoding_for_model(model_name)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "def handle_error(response, retry_count, max_retry_attempts, retry_wait_time):\n",
        "    if response.status_code in {429, 500, 503} and retry_count < max_retry_attempts:\n",
        "        print(f\"Retrying after {retry_wait_time} seconds...\")\n",
        "        time.sleep(retry_wait_time)\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "QfbZLRJlsWk2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat Model Wrapper\n",
        "class ChatModelWrapper:\n",
        "    def __init__(self, api_key, use_memory=True, max_completion_token=2000, model_name=\"gpt-3.5-turbo\"):\n",
        "        openai.api_key = api_key\n",
        "        self.memory = []\n",
        "        self.max_completion_token = max_completion_token\n",
        "        self.model_name = model_name\n",
        "        self.use_memory = use_memory\n",
        "\n",
        "    def _manage_memory(self, current_prompt_content, max_tokens):\n",
        "        total_required_token = self.max_completion_token - (count_tokens(current_prompt_content) + max_tokens)\n",
        "        combined_memory_tokens = sum(count_tokens(msg[\"role\"] + msg[\"content\"]) for msg in self.memory)\n",
        "        removed_messages = []\n",
        "\n",
        "        while combined_memory_tokens > total_required_token:\n",
        "            removed_message = self.memory.pop(0)\n",
        "            combined_memory_tokens -= count_tokens(removed_message[\"content\"])\n",
        "            removed_messages.append(removed_message)\n",
        "\n",
        "    def _generate_prompt(self, messages, max_tokens):\n",
        "        if self.use_memory:\n",
        "            current_prompt_content = \" \".join(message[\"content\"] for message in messages)\n",
        "            self._manage_memory(current_prompt_content, max_tokens)\n",
        "            final_messages = self.memory + messages\n",
        "        else:\n",
        "            final_messages = messages\n",
        "        return final_messages\n",
        "\n",
        "    def _chat_completion(self, messages: List[ChatMessage], max_tokens: int = 128, **kwargs) -> openai.ChatCompletion:\n",
        "        retry_count = 0\n",
        "\n",
        "        total_tokens = sum(count_tokens(msg[\"content\"]) for msg in messages)\n",
        "        if total_tokens + max_tokens > self.max_completion_token:\n",
        "            return \"Error: Total tokens exceed the limit.\"\n",
        "\n",
        "        while retry_count < kwargs.get(\"max_retry_attempts\", 3):\n",
        "            try:\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=kwargs.get(\"model\", self.model_name),\n",
        "                    messages=messages,\n",
        "                    max_tokens=max_tokens,\n",
        "                    **kwargs\n",
        "                )\n",
        "                return response\n",
        "\n",
        "            except openai.error.OpenAIError as e:\n",
        "                if handle_error(e, retry_count, kwargs.get(\"max_retry_attempts\", 3), kwargs.get(\"retry_wait_time\", 60)):\n",
        "                    print(\"Error in _chat_completion: \", e)\n",
        "                    retry_count += 1\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "    def generate_response(self, messages: List[ChatMessage], max_tokens: int = 128, **kwargs) -> openai.ChatCompletion:\n",
        "        if len(messages) == 0:\n",
        "            return \"Error: No input messages.\"\n",
        "\n",
        "        prompt = self._generate_prompt(messages, max_tokens)\n",
        "        response = self._chat_completion(prompt, **kwargs)\n",
        "\n",
        "        try:\n",
        "            if response and response.choices and len(response.choices) > 0:\n",
        "                if self.use_memory:\n",
        "                    self.memory.extend(messages)\n",
        "                    response_dict = {\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": response.choices[0].message[\"content\"]\n",
        "                    }\n",
        "                    self.memory.append(response_dict)\n",
        "                return response\n",
        "        except:\n",
        "            return response\n",
        "\n",
        "    def set_model(self, model_name):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def set_memory_usage(self, use_memory):\n",
        "        self.use_memory = use_memory\n",
        "\n",
        "    def prioritize_messages(self, messages: List[ChatMessage]):\n",
        "        # Sort messages based on timestamp or relevance score\n",
        "        # Ensure that higher-priority messages come first in the list\n",
        "        sorted_messages = sorted(messages, key=lambda msg: msg.get(\"timestamp\", 0), reverse=True)\n",
        "        return sorted_messages\n",
        "\n",
        "    def split_long_conversation(self, messages: List[ChatMessage], max_tokens_per_chunk):\n",
        "        split_chunks = []\n",
        "        current_chunk = []\n",
        "        current_chunk_tokens = 0\n",
        "\n",
        "        for msg in messages:\n",
        "            msg_tokens = count_tokens(msg[\"content\"])\n",
        "            if current_chunk_tokens + msg_tokens <= max_tokens_per_chunk:\n",
        "                current_chunk.append(msg)\n",
        "                current_chunk_tokens += msg_tokens\n",
        "            else:\n",
        "                split_chunks.append(current_chunk)\n",
        "                current_chunk = [msg]\n",
        "                current_chunk_tokens = msg_tokens\n",
        "\n",
        "        if current_chunk:\n",
        "            split_chunks.append(current_chunk)\n",
        "\n",
        "        return split_chunks"
      ],
      "metadata": {
        "id": "42rBoxf5o8Ew"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Completion Model Wrapper\n",
        "class CompletionModelWrapper:\n",
        "    def __init__(self, api_key, use_memory=True, max_completion_token=3000, model_name=\"text-davinci-003\"):\n",
        "        openai.api_key = api_key\n",
        "        self.memories = []\n",
        "        self.max_completion_token = max_completion_token\n",
        "        self.model_name = model_name\n",
        "        self.use_memory = use_memory\n",
        "        self.max_retry_attempts = 3\n",
        "\n",
        "    def _manage_memory(self, current_prompt, max_tokens):\n",
        "        number_of_token_in_current_prompt = count_tokens(current_prompt, model_name=\"text-davinci-003\")\n",
        "        total_memory_tokens = sum(count_tokens(memory[\"USER\"], model_name=\"text-davinci-003\") + count_tokens(memory[\"AI\"], model_name=\"text-davinci-003\") for memory in self.memories)\n",
        "\n",
        "        while total_memory_tokens > self.max_completion_token - (number_of_token_in_current_prompt + max_tokens):\n",
        "            removed_memory = self.memories.pop(0)\n",
        "            total_memory_tokens -= count_tokens(removed_memory[\"USER\"], model_name=\"text-davinci-003\") + count_tokens(removed_memory[\"AI\"], model_name=\"text-davinci-003\")\n",
        "\n",
        "    def _format_conversation(self, current_prompt):\n",
        "        if self.use_memory:\n",
        "            conversation_series = \"\\n\".join([f\"User: {memory['USER']}\\nAI: {memory['AI']}\" for memory in self.memories])\n",
        "            conversation_series += f\"\\nUser: {current_prompt}\\nAI:\"\n",
        "            return conversation_series\n",
        "        else:\n",
        "            return current_prompt\n",
        "\n",
        "    def _completion(self, prompt: str, max_tokens: int = 2000, temperature=1.0, **kwargs) -> openai.Completion:\n",
        "        prompt_with_memory = self._format_conversation(prompt)\n",
        "        retry_count = 0\n",
        "        while retry_count < self.max_retry_attempts:\n",
        "            try:\n",
        "                response = openai.Completion.create(\n",
        "                    model=self.model_name,\n",
        "                    prompt=prompt_with_memory,\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature,\n",
        "                    **kwargs\n",
        "                )\n",
        "                return response\n",
        "            except openai.error.OpenAIError as e:\n",
        "                if handle_error(e, retry_count, self.max_retry_attempts, kwargs.get(\"retry_wait_time\", 60)):\n",
        "                    print(\"Error in _completion: \", e)\n",
        "                    retry_count += 1\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "    def generate_response(self, prompt: str, max_tokens: int = 2000, temperature=1.0, **kwargs) -> openai.Completion:\n",
        "        res = self._completion(prompt, max_tokens, temperature, **kwargs)\n",
        "        if res:\n",
        "            memory = {\n",
        "                \"USER\": prompt,\n",
        "                \"AI\": res.choices[0].text.strip()\n",
        "            }\n",
        "            self.memories.append(memory)\n",
        "            self._manage_memory(prompt, max_tokens)  # Dynamic memory management\n",
        "        return res\n",
        "\n",
        "    def set_model(self, model_name):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def set_memory_usage(self, use_memory):\n",
        "        self.use_memory = use_memory"
      ],
      "metadata": {
        "id": "dlIWrPSgtdtV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Wrapper\n",
        "class LLMWrapper:\n",
        "    def __init__(self, api_key, model_type, use_memory=True, max_chat_completion_token=3000, model_name=\"gpt-3.5-turbo\"):\n",
        "        self.api_key = api_key\n",
        "        self.model_type = model_type\n",
        "        self.use_memory = use_memory\n",
        "        self.max_chat_completion_token = max_chat_completion_token\n",
        "        self.model_name = model_name\n",
        "        self.chat_wrapper = ChatModelWrapper(self.api_key, self.use_memory, self.max_chat_completion_token, self.model_name)\n",
        "        self.completion_wrapper = CompletionModelWrapper(self.api_key, self.use_memory, self.max_chat_completion_token, self.model_name)\n",
        "\n",
        "    def generate_response(self, messages: List[ChatMessage], max_tokens: int = 2000, **kwargs) -> Union[openai.ChatCompletion, openai.Completion, str]:\n",
        "        if self.model_type == \"Chat\":\n",
        "            res = self.chat_wrapper.generate_response(messages, max_tokens, **kwargs)\n",
        "            return res.choices[0].message\n",
        "        elif self.model_type == \"Completion\":\n",
        "            res = self.completion_wrapper.generate_response(messages, max_tokens, **kwargs)\n",
        "            return res.choices[0].text.strip()\n",
        "        else:\n",
        "            return \"Invalid model_type specified.\""
      ],
      "metadata": {
        "id": "bTZZvVWgtfMV"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chatbot Test using LLMWrapper"
      ],
      "metadata": {
        "id": "lWCyeSnZwbWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of your LLMWrapper for chat-based interactions\n",
        "chatbot = LLMWrapper(API_KEY, model_type='Chat')\n",
        "\n",
        "print(\"Chatbot: Hello! How can I assist you today?\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    user_message = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_input\n",
        "    }\n",
        "\n",
        "    response = chatbot.generate_response([user_message], max_tokens=128, temperature=0.7)\n",
        "    print(f\"Chatbot: {response.content}\")"
      ],
      "metadata": {
        "id": "Os0ps_OIuqjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section Wise Article Generator Test using LLMWrapper"
      ],
      "metadata": {
        "id": "BJ5i5DQPwlaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test functions\n",
        "def generate_metadata_sections():\n",
        "    chat_wrapper = ChatModelWrapper(API_KEY)\n",
        "\n",
        "    metadata_tasks = [\n",
        "        {\"role\": \"user\", \"content\": \"Please provide an introduction for a travel destination.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Describe the local cuisine and dining experiences.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Highlight must-visit attractions on the island.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Share insights about the island's culture and traditions.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Provide recommendations for accommodations and lodging.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Conclude the article with a summary of Santorini's charm.\"}\n",
        "    ]\n",
        "\n",
        "    metadata_sections = []\n",
        "    for task in metadata_tasks:\n",
        "        chat_response = chat_wrapper.generate_response([task], max_tokens=128, temperature=0.7)\n",
        "        metadata_sections.append(chat_response.choices[0].message[\"content\"])\n",
        "\n",
        "    return metadata_sections\n",
        "\n",
        "def write_article_sections(metadata_sections):\n",
        "    completion_wrapper = CompletionModelWrapper(API_KEY)\n",
        "\n",
        "    article_prompts = metadata_sections\n",
        "\n",
        "    article_sections = []\n",
        "    for prompt in article_prompts:\n",
        "        system_prompt = \"Generate an article section about the following topic:\"\n",
        "        full_prompt = f\"{system_prompt}\\n{prompt}\"\n",
        "\n",
        "        completion_response = completion_wrapper.generate_response(full_prompt, max_tokens=800, temperature=0.7)\n",
        "        article_sections.append(completion_response.choices[0].text.strip())\n",
        "\n",
        "    return article_sections"
      ],
      "metadata": {
        "id": "6-oRIxWWtgvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    metadata_sections = generate_metadata_sections()\n",
        "    article_sections = write_article_sections(metadata_sections)\n",
        "\n",
        "    for i, (metadata, section) in enumerate(zip(metadata_sections, article_sections), start=1):\n",
        "        print(f\"Section {i} - Metadata:\")\n",
        "        print(metadata)\n",
        "        print(\"==\" * 40)\n",
        "        print(f\"Section {i} - Section Content:\")\n",
        "        print(section)\n",
        "        print(\"==\" * 40)"
      ],
      "metadata": {
        "id": "FZtrSrcduJXa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}