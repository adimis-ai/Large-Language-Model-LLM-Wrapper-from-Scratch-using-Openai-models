{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCrZMzGHxQ/w6BCOjh/2Pu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adimis-ai/Large-Language-Model-LLM-Wrapper-from-Scratch-using-Openai-models/blob/main/Large_Language_Model_(LLM)_Wrapper_from_Scratch_using_Openai_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "VBLvTBLxP32D"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install --upgrade tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import tiktoken\n",
        "import time\n",
        "from typing import List, Union\n",
        "from pydantic import BaseModel\n",
        "import json"
      ],
      "metadata": {
        "id": "C4SmZWvcsQ9l"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"YOUR_OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "Hkzjm1gYsSI8"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions\n",
        "def count_tokens(text, model_name=\"gpt-3.5-turbo\"):\n",
        "    encoding = tiktoken.encoding_for_model(model_name)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "def handle_error(response, retry_count, max_retry_attempts, retry_wait_time):\n",
        "    if response.status_code in {429, 500, 503} and retry_count < max_retry_attempts:\n",
        "        print(f\"Retrying after {retry_wait_time} seconds...\")\n",
        "        time.sleep(retry_wait_time)\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "QfbZLRJlsWk2"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatMessage(BaseModel):\n",
        "    role: str\n",
        "    content: str\n",
        "\n",
        "class ChatMessageEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, ChatMessage):\n",
        "            return obj.__dict__  # Convert ChatMessage object to a dictionary\n",
        "        elif isinstance(obj, list) and all(isinstance(item, ChatMessage) for item in obj):\n",
        "            return [item.__dict__ for item in obj]  # Convert list of ChatMessage objects to a list of dictionaries\n",
        "        return super().default(obj)\n",
        "\n",
        "class ChatModelWrapper:\n",
        "    def __init__(self, api_key, use_memory=True, max_completion_token=2000, model_name=\"gpt-3.5-turbo\"):\n",
        "        openai.api_key = api_key\n",
        "        self.memory = []\n",
        "        self.max_completion_token = max_completion_token\n",
        "        self.model_name = model_name\n",
        "        self.use_memory = use_memory\n",
        "\n",
        "    def _manage_memory(self, current_prompt_content, max_tokens):\n",
        "        total_required_token = self.max_completion_token - (count_tokens(current_prompt_content) + max_tokens)\n",
        "        combined_memory_tokens = sum(count_tokens(msg.content) for msg in self.memory)\n",
        "        removed_messages = []\n",
        "\n",
        "        while combined_memory_tokens > total_required_token:\n",
        "            removed_message = self.memory.pop(0)\n",
        "            combined_memory_tokens -= count_tokens(removed_message.content)\n",
        "            removed_messages.append(removed_message)\n",
        "\n",
        "    def _generate_prompt(self, messages, max_tokens):\n",
        "        if self.use_memory:\n",
        "            current_prompt_content = \" \".join(message.content for message in messages)\n",
        "            self._manage_memory(current_prompt_content, max_tokens)\n",
        "            final_messages = self.memory + messages\n",
        "        else:\n",
        "            final_messages = messages\n",
        "        return final_messages\n",
        "\n",
        "    def _chat_completion(self, messages: List[ChatMessage], max_tokens: int = 128, **kwargs) -> openai.ChatCompletion:\n",
        "        retry_count = 0\n",
        "\n",
        "        total_tokens = sum(count_tokens(msg.content) for msg in messages)\n",
        "        if total_tokens + max_tokens > self.max_completion_token:\n",
        "            return \"Error: Total tokens exceed the limit.\"\n",
        "\n",
        "        while retry_count < kwargs.get(\"max_retry_attempts\", 3):\n",
        "            try:\n",
        "                json_string = json.dumps(messages, cls=ChatMessageEncoder)\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=kwargs.get(\"model\", self.model_name),\n",
        "                    messages=json.loads(json_string),  # Load JSON string as a list\n",
        "                    max_tokens=max_tokens,\n",
        "                    **kwargs\n",
        "                )\n",
        "                return response\n",
        "\n",
        "            except openai.error.OpenAIError as e:\n",
        "                if handle_error(e, retry_count, kwargs.get(\"max_retry_attempts\", 3), kwargs.get(\"retry_wait_time\", 60)):\n",
        "                    print(\"Error in _chat_completion: \", e)\n",
        "                    retry_count += 1\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "    def generate_response(self, messages: List[ChatMessage], max_tokens: int = 128, **kwargs) -> openai.ChatCompletion:\n",
        "        if len(messages) == 0:\n",
        "            return \"Error: No input messages.\"\n",
        "\n",
        "        prompt = self._generate_prompt(messages, max_tokens)\n",
        "\n",
        "        response = self._chat_completion(prompt, **kwargs)\n",
        "\n",
        "        try:\n",
        "            if response and response.choices and len(response.choices) > 0:\n",
        "                if self.use_memory:\n",
        "                    self.memory.extend(messages)\n",
        "                    response_dict = {\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": response.choices[0].message.content\n",
        "                    }\n",
        "                    self.memory.append(response_dict)\n",
        "                return response\n",
        "        except:\n",
        "            return response\n",
        "\n",
        "    def set_model(self, model_name):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def set_memory_usage(self, use_memory):\n",
        "        self.use_memory = use_memory\n",
        "\n",
        "    def prioritize_messages(self, messages: List[ChatMessage]):\n",
        "        # Sort messages based on timestamp or relevance score\n",
        "        # Ensure that higher-priority messages come first in the list\n",
        "        sorted_messages = sorted(messages, key=lambda msg: msg.timestamp, reverse=True)\n",
        "        return sorted_messages\n",
        "\n",
        "    def split_long_conversation(self, messages: List[ChatMessage], max_tokens_per_chunk):\n",
        "        split_chunks = []\n",
        "        current_chunk = []\n",
        "        current_chunk_tokens = 0\n",
        "\n",
        "        for msg in messages:\n",
        "            msg_tokens = count_tokens(msg.content)\n",
        "            if current_chunk_tokens + msg_tokens <= max_tokens_per_chunk:\n",
        "                current_chunk.append(msg)\n",
        "                current_chunk_tokens += msg_tokens\n",
        "            else:\n",
        "                split_chunks.append(current_chunk)\n",
        "                current_chunk = [msg]\n",
        "                current_chunk_tokens = msg_tokens\n",
        "\n",
        "        if current_chunk:\n",
        "            split_chunks.append(current_chunk)\n",
        "\n",
        "        return split_chunks"
      ],
      "metadata": {
        "id": "42rBoxf5o8Ew"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Completion Model Wrapper\n",
        "class CompletionModelWrapper:\n",
        "    def __init__(self, api_key, use_memory=True, max_completion_token=3000, model_name=\"text-davinci-003\"):\n",
        "        openai.api_key = api_key\n",
        "        self.memories = []\n",
        "        self.max_completion_token = max_completion_token\n",
        "        self.completion_model_name = model_name\n",
        "        self.use_memory = use_memory\n",
        "        self.max_retry_attempts = 3\n",
        "\n",
        "    def _manage_memory(self, current_prompt, max_tokens):\n",
        "        number_of_token_in_current_prompt = count_tokens(current_prompt, model_name=self.completion_model_name)\n",
        "        total_memory_tokens = sum(count_tokens(memory[\"USER\"], model_name=self.completion_model_name) + count_tokens(memory[\"AI\"], model_name=self.completion_model_name) for memory in self.memories)\n",
        "\n",
        "        while total_memory_tokens > self.max_completion_token - (number_of_token_in_current_prompt + max_tokens):\n",
        "            removed_memory = self.memories.pop(0)\n",
        "            total_memory_tokens -= count_tokens(removed_memory[\"USER\"], model_name=self.completion_model_name) + count_tokens(removed_memory[\"AI\"], model_name=self.completion_model_name)\n",
        "\n",
        "    def _format_conversation(self, current_prompt):\n",
        "        if self.use_memory:\n",
        "            conversation_series = \"\\n\".join([f\"User: {memory['USER']}\\nAI: {memory['AI']}\" for memory in self.memories])\n",
        "            conversation_series += f\"\\nUser: {current_prompt}\\nAI:\"\n",
        "            return conversation_series\n",
        "        else:\n",
        "            return current_prompt\n",
        "\n",
        "    def _completion(self, prompt: str, max_tokens: int = 2000, temperature=1.0, **kwargs) -> openai.Completion:\n",
        "        prompt_with_memory = self._format_conversation(prompt)\n",
        "        retry_count = 0\n",
        "        while retry_count < self.max_retry_attempts:\n",
        "            try:\n",
        "                response = openai.Completion.create(\n",
        "                    model=self.completion_model_name,\n",
        "                    prompt=prompt_with_memory,\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature,  # Control randomness of output\n",
        "                    **kwargs\n",
        "                )\n",
        "                return response\n",
        "            except openai.error.OpenAIError as e:\n",
        "                if handle_error(e.response, retry_count, self.max_retry_attempts, kwargs.get(\"retry_wait_time\", 60)):\n",
        "                    retry_count += 1\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "    def generate_response(self, prompt: str, max_tokens: int = 2000, temperature=1.0, **kwargs) -> openai.Completion:\n",
        "        res = self._completion(prompt, max_tokens, temperature, **kwargs)\n",
        "        if res:\n",
        "            memory = {\n",
        "                \"USER\": prompt,\n",
        "                \"AI\": res.choices[0].text.strip()\n",
        "            }\n",
        "            self.memories.append(memory)\n",
        "            self._manage_memory(prompt, max_tokens)  # Dynamic memory management\n",
        "        return res\n",
        "\n",
        "    def set_model(self, model_name):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def set_memory_usage(self, use_memory):\n",
        "        self.use_memory = use_memory"
      ],
      "metadata": {
        "id": "dlIWrPSgtdtV"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Wrapper\n",
        "class LLMWrapper:\n",
        "    def __init__(self, api_key, model_type, use_memory=True, max_chat_completion_token=3000, model_name=\"gpt-3.5-turbo\", completion_model_name=\"text-davinci-003\"):\n",
        "        self.api_key = api_key\n",
        "        self.model_type = model_type\n",
        "        self.use_memory = use_memory\n",
        "        self.max_chat_completion_token = max_chat_completion_token\n",
        "        self.model_name = model_name\n",
        "        self.completion_model_name = completion_model_name\n",
        "        self.chat_wrapper = ChatModelWrapper(self.api_key, self.use_memory, self.max_chat_completion_token, self.model_name)\n",
        "        self.completion_wrapper = CompletionModelWrapper(self.api_key, self.use_memory, self.max_chat_completion_token, self.completion_model_name)\n",
        "\n",
        "    def generate_response(self, messages_or_prompt, max_tokens: int = 2000, **kwargs) -> Union[openai.ChatCompletion, openai.Completion, str]:\n",
        "        if isinstance(messages_or_prompt, str):\n",
        "            prompt = messages_or_prompt\n",
        "            messages = [ChatMessage(role=\"user\", content=prompt)]\n",
        "        else:\n",
        "            messages = messages_or_prompt\n",
        "            prompt = messages[-1].content\n",
        "\n",
        "        if self.model_type == \"Chat\":\n",
        "            res = self.chat_wrapper.generate_response(messages, max_tokens, **kwargs)\n",
        "            return res.choices[0].message\n",
        "        elif self.model_type == \"Completion\":\n",
        "            res = self.completion_wrapper.generate_response(prompt, max_tokens, **kwargs)\n",
        "            return res.choices[0].text.strip()\n",
        "        else:\n",
        "            return \"Invalid model_type specified.\""
      ],
      "metadata": {
        "id": "bTZZvVWgtfMV"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_wrapper = LLMWrapper(API_KEY, model_type=\"Chat\")\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Hello, how are you?\"),\n",
        "    ChatMessage(role=\"assistant\", content=\"I'm doing well, thank you!\")\n",
        "]\n",
        "\n",
        "response = llm_wrapper.generate_response(messages, max_tokens=100)\n",
        "print(\"Generated Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WBSbwIPz4HF",
        "outputId": "734e2fd3-9f25-4faf-839d-8c81a1c57a1a"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Response: {\n",
            "  \"role\": \"assistant\",\n",
            "  \"content\": \"Hello! I'm an AI and I don't have feelings, but thank you for asking. How can I assist you today?\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LLMWrapper with the model type \"Completion\"\n",
        "llm_wrapper = LLMWrapper(API_KEY, model_type=\"Completion\")\n",
        "\n",
        "# Define the prompt for generating completion\n",
        "prompt = \"Once upon a time in a land far, far away\"\n",
        "\n",
        "# Generate a response using the provided prompt\n",
        "response = llm_wrapper.generate_response(prompt)\n",
        "\n",
        "# Print the generated response\n",
        "print(\"Generated Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTSOfrS248GS",
        "outputId": "9eef868f-e806-477b-d48e-358f63b022a3"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Response: There lived a young maiden who dreamed of one day finding her true love. She searched near and far to find someone who could make her feel loved and appreciated. With a bit of luck, she finally found her Prince Charming. She lived happily ever after.\n"
          ]
        }
      ]
    }
  ]
}